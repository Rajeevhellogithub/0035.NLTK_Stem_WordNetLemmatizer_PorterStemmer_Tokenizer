{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Completed in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mx9-RA-KzoWb",
    "outputId": "1b12fb5a-8376-4f85-d0b3-1b516eccc0c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fw96ma4Z0OPN"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svRtqChe0r4k",
    "outputId": "e88ec0e8-e2b5-4263-8cef-696b41c7b4f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,    11,   703,   389,   345,    30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "uhXUmRj3009Z"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_wyl-5m1FSk",
    "outputId": "ac895e30-c6a7-4cfe-eff8-93fc22977039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Once upon a time\", return_tensors='pt')\n",
    "output = model.generate(input_ids, pad_token_id=tokenizer.pad_token_id, max_length=50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "xgjqLFyN4CDk"
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, max_length=512):\n",
    "    tokens = tokenizer.encode(text, return_tensors='pt')[0]\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Vi_sN1aW4nkb"
   },
   "outputs": [],
   "source": [
    "def generate_responses(chunks):\n",
    "    responses = []\n",
    "    for chunk in chunks:\n",
    "        input_ids = chunk.unsqueeze(0)\n",
    "        output = model.generate(input_ids, pad_token_id=tokenizer.pad_token_id, max_length=512)\n",
    "        responses.append(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "sh4RPeE14sfS"
   },
   "outputs": [],
   "source": [
    "long_text = \"brief explain about generative ai \" * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62sJQBT04xdY",
    "outputId": "7cc17852-d3fd-4b6e-aa01-53fe5474389d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  65, 3796, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,\n",
       "          876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,\n",
       "          546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72,\n",
       "         4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,\n",
       "          257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546,\n",
       "         1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506,\n",
       "         4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,\n",
       "           72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,\n",
       "          876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,\n",
       "          546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72,\n",
       "         4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,\n",
       "          257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546,\n",
       "         1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506,\n",
       "         4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,\n",
       "           72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,\n",
       "          876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,\n",
       "          546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72,\n",
       "         4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,\n",
       "          257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546,\n",
       "         1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506,\n",
       "         4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,\n",
       "           72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,\n",
       "          876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,\n",
       "          546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72,\n",
       "         4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,\n",
       "          257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546,\n",
       "         1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,   72, 4506,\n",
       "         4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,  876,  257,\n",
       "           72, 4506, 4727,  546, 1152,  876,  257,   72, 4506, 4727,  546, 1152,\n",
       "          876,  257,   72,  220])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_text(long_text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkGkTCfC4xmK",
    "outputId": "f7ae2296-da34-40bf-ca41-2c57b2b4a306"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai \\xa0explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = generate_responses(chunks)\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pY0nhTxE4xsE",
    "outputId": "e0075c32-24e4-43e8-e3ed-2b550815f7c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for chunk 1:\n",
      "brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai  explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative ai brief explain about generative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, response in enumerate(responses):\n",
    "  print(f\"Response for chunk {i+1}:\\n{response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
